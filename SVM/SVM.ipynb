{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM)\n",
    "\n",
    "SVM (Support Vector Machine) is a classification algorithm that finds the optimal boundary (hyperplane) to separate different classes. It is useful when data is high-dimensional and works well even when classes are not linearly separable.\n",
    "\n",
    "### Finding the Optimal Hyperplane\n",
    "\n",
    "SVM finds a hyperplane that best separates two classes while maximizing the margin, which is the distance between the hyperplane and the closest points from each class, called support vectors.\n",
    "\n",
    "For a dataset with two features like age and fare, the hyperplane is a line that separates survivors from non-survivors.\n",
    "\n",
    "\n",
    "### Handling Non-Linearly Separable Data\n",
    "\n",
    "If data is not linearly separable, SVM applies a kernel trick to transform it into a higher-dimensional space where separation is possible.\n",
    "\n",
    "Examples of kernels:\n",
    "\n",
    "- Linear Kernel works for simple separable data.\n",
    "- Polynomial Kernel maps features to higher dimensions.\n",
    "- RBF Kernel creates complex boundaries for non-linearly separable data.\n",
    "\n",
    "![alt text](<svm_kernel.png>)\n",
    "### Using Soft Margin with the C Parameter\n",
    "\n",
    "If data is noisy, SVM allows some misclassifications using a soft margin, which is controlled by the C parameter.\n",
    "\n",
    "- High C results in fewer misclassifications but may overfit.\n",
    "- Low C allows more misclassification for better generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Training SVM on Titanic Dataset\n",
    "\n",
    "Using the Titanic dataset, train an SVM classifier to predict survival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "file_path = \"../Datasets/Titanic_dataset_train.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df[\"Age\"] = df[\"Age\"].fillna(df[\"Age\"].median())\n",
    "df[\"Embarked\"] = df[\"Embarked\"].fillna(df[\"Embarked\"].mode()[0])\n",
    "df.drop(columns=[\"Cabin\"], inplace=True)\n",
    "df[\"Sex\"] = df[\"Sex\"].map({\"male\": 0, \"female\": 1})\n",
    "\n",
    "df = pd.get_dummies(df, columns=[\"Embarked\"], drop_first=True)\n",
    "\n",
    "features = [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"SibSp\",  \"Parch\", \"Embarked_S\", \"Embarked_Q\"]\n",
    "\n",
    "X = df[features]\n",
    "y = df[\"Survived\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.6536\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.94      0.76       105\n",
      "           1       0.75      0.24      0.37        74\n",
      "\n",
      "    accuracy                           0.65       179\n",
      "   macro avg       0.69      0.59      0.56       179\n",
      "weighted avg       0.68      0.65      0.60       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train SVM classifier with RBF kernel\n",
    "svm = SVC(kernel=\"rbf\", C=1.0, gamma=\"scale\", random_state=42)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "report_svm = classification_report(y_test, y_pred_svm)\n",
    "\n",
    "print(f\"SVM Accuracy: {accuracy_svm:.4f}\")\n",
    "print(\"Classification Report:\\n\", report_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Key Parameters in SVM\n",
    "\n",
    "- `kernel` determines the type of decision boundary. Options include `linear`, `polynomial`, and `RBF`.\n",
    "- `C` controls the margin. A high value makes the model strict with fewer misclassifications, while a low value allows more misclassifications for better generalization.\n",
    "- `gamma` is used in non-linear kernels to define the influence of each training point.\n",
    "\n",
    "### Comparison of SVM with Decision Tree and Random Forest\n",
    "\n",
    "| Feature                  | SVM                     | Decision Tree          | Random Forest         |\n",
    "|--------------------------|------------------------|------------------------|------------------------|\n",
    "| Works with Non-Linear Data | Yes (with kernel trick) | No                     | Yes                    |\n",
    "| Feature Scaling Needed   | Yes                     | No                     | No                     |\n",
    "| Handles Large Datasets   | No (slower)             | Yes                    | Yes                    |\n",
    "| Overfitting Risk         | High (if C is too high) | High (if deep)         | Low                    |\n",
    "| Interpretability         | Low                     | High                   | Medium                 |\n",
    "| Speed on Large Data      | Slow                    | Fast                   | Fast                   |\n",
    "\n",
    "### Comparison of SVM with Logistic Regression\n",
    "\n",
    "SVM and Logistic Regression are both used for binary classification and attempt to separate two classes, but they work differently.\n",
    "\n",
    "| Feature                  | SVM                     | Logistic Regression    |\n",
    "|--------------------------|------------------------|------------------------|\n",
    "| Decision Boundary        | Can be linear or non-linear | Always linear        |\n",
    "| Works with Non-Linearity | Yes (with kernel trick) | No                     |\n",
    "| Feature Scaling Needed   | Yes                     | Yes                    |\n",
    "| Handles Large Datasets   | No (slower)             | Yes                    |\n",
    "| Sensitivity to Outliers  | High                    | Moderate               |\n",
    "| Probability Outputs      | No                      | Yes (predicts probabilities) |\n",
    "\n",
    "Logistic Regression always finds a linear decision boundary, while SVM can find non-linear boundaries using kernels. SVM is more flexible but computationally expensive for large datasets. Logistic Regression is faster and provides probability scores, which makes it useful when estimating uncertainty in predictions.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "SVM is a powerful model for classification, especially when data is not linearly separable. It works best with proper feature scaling. It is slower on large datasets compared to decision trees and random forests. Choosing the right kernel and hyperparameters is crucial for good performance.\n",
    "\n",
    "SVM is a good choice for small datasets where complex decision boundaries are needed. Logistic Regression is more efficient when working with large datasets and when interpretability is important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.8212\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.90      0.86       105\n",
      "           1       0.84      0.70      0.76        74\n",
      "\n",
      "    accuracy                           0.82       179\n",
      "   macro avg       0.83      0.80      0.81       179\n",
      "weighted avg       0.82      0.82      0.82       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Scale features (SVM is sensitive to feature scaling)\n",
    "scaler = StandardScaler()\n",
    "X_train_scale = scaler.fit_transform(X_train)\n",
    "X_test_scale = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and train SVM classifier with RBF kernel\n",
    "svm = SVC(kernel=\"rbf\", C=1.0, gamma=\"scale\", random_state=42)\n",
    "svm.fit(X_train_scale, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_svm = svm.predict(X_test_scale)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "report_svm = classification_report(y_test, y_pred_svm)\n",
    "\n",
    "print(f\"SVM Accuracy: {accuracy_svm:.4f}\")\n",
    "print(\"Classification Report:\\n\", report_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Scaling Matters for SVM\n",
    "\n",
    "Support Vector Machine (SVM) is sensitive to feature scaling because it uses distance-based calculations to determine the optimal decision boundary (hyperplane). If features have very different scales, some features may dominate others, affecting the model's performance.\n",
    "\n",
    "### Example: Titanic Dataset Features\n",
    "\n",
    "Consider the two features in the Titanic dataset:\n",
    "- **Age**: Ranges from around 0 to 80.\n",
    "- **Fare**: Ranges from 0 to 500+.\n",
    "\n",
    "Since Fare has much larger values compared to Age, the SVM model prioritizes Fare over Age when finding the hyperplane. This leads to incorrect margins and poor classification performance.\n",
    "\n",
    "## How SVM Uses Distance\n",
    "\n",
    "SVM finds a hyperplane by maximizing the margin between two classes. This involves calculating distances using formulas like:\n",
    "\n",
    "$\\text{Euclidean Distance} = \\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}$\n",
    "\n",
    "If one feature (like Fare) has values much larger than another (like Age), the distance calculations become biased toward the larger feature, causing incorrect decision boundaries.\n",
    "\n",
    "## Effect of Scaling Before and After\n",
    "\n",
    "### Without Scaling (Unscaled Data)\n",
    "- Features like Fare (0-500) dominate the SVM model.\n",
    "- SVM may learn a hyperplane that ignores Age.\n",
    "- Decision boundary becomes skewed, leading to poor predictions.\n",
    "\n",
    "### With Scaling (Standardized Data)\n",
    "- All features are transformed to a similar range (e.g., mean 0, variance 1).\n",
    "- SVM treats all features equally when computing distances.\n",
    "- The margin is correctly optimized, leading to better classification accuracy.\n",
    "\n",
    "## Scaling Methods for SVM\n",
    "\n",
    "### Standardization (Recommended)\n",
    "- Converts data to mean 0 and variance 1:\n",
    "\n",
    "\n",
    "$x_{\\text{scaled}} = \\frac{x - \\text{mean}(x)}{\\text{std}(x)}$\n",
    "\n",
    "- Used with SVM (RBF, Polynomial kernels).\n",
    "\n",
    "\n",
    "### Min-Max Scaling (Alternative)\n",
    "- Rescales features between 0 and 1:\n",
    "\n",
    "$x_{\\text{scaled}} = \\frac{x - \\\\min(x)}{\\max(x) - \\min(x)}$\n",
    "\n",
    "- Useful for SVM with linear kernels.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "- SVM relies on distances, so unscaled features distort decision boundaries.\n",
    "- Standardization ensures equal weight for all features, leading to better model performance.\n",
    "- Always scale features before using SVM, especially when features have different units or ranges.\n",
    "\n",
    "Scaling is essential for ensuring that SVM makes fair and accurate decisions across all features in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitivity of Machine Learning Algorithms to Feature Scaling\n",
    "\n",
    "| Algorithm            | Sensitive to Scaling? | Reason |\n",
    "|----------------------|----------------------|--------|\n",
    "| **Linear Regression**  | Yes  | Uses distance-based optimization; large feature values can dominate the model. |\n",
    "| **Logistic Regression**  | Yes  | Uses gradient-based optimization; feature magnitudes affect convergence speed and decision boundary. |\n",
    "| **Naive Bayes**  | No  | Based on probabilities and assumes independent features; scaling does not impact calculations. |\n",
    "| **K-Nearest Neighbors (KNN)**  | Yes  | Uses Euclidean distance to find neighbors; large-scale features dominate distance calculations. |\n",
    "| **Decision Tree**  | No  | Splits data based on feature values; scaling does not change the relative order of feature values. |\n",
    "| **Random Forest**  | No  | Uses multiple decision trees; feature magnitudes do not impact splits. |\n",
    "| **Support Vector Machine (SVM)**  | Yes  | Finds a hyperplane using distance calculations; unscaled features distort the margin. |\n",
    "| **Gradient Boosting (XGBoost, LightGBM, CatBoost)** | No  | Works with decision trees, which are not affected by feature magnitudes. |\n",
    "| **Neural Networks (MLP, Deep Learning)** | Yes  | Uses gradient descent; unscaled inputs slow training and affect weight updates. |\n",
    "\n",
    "## Summary\n",
    "\n",
    "Algorithms that use **distance-based calculations** or **gradient-based optimization** are affected by scaling. These include linear regression, logistic regression, KNN, SVM, and neural networks. \n",
    "\n",
    "Algorithms based on **tree-based models** such as decision trees, random forests, and boosting methods are not sensitive to scaling since they split data based on thresholds rather than distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Evaluating Models on Unscaled Data =====\n",
      "KNN Accuracy: 0.7095\n",
      "Logistic Regression Accuracy: 0.8101\n",
      "Naive Bayes Accuracy: 0.7709\n",
      "Random Forest Accuracy: 0.8045\n",
      "Decision Tree Accuracy: 0.7821\n",
      "SVM Accuracy: 0.6536\n",
      "\n",
      "===== Evaluating Models on Scaled Data =====\n",
      "KNN Accuracy: 0.8045\n",
      "Logistic Regression Accuracy: 0.8101\n",
      "Naive Bayes Accuracy: 0.7709\n",
      "Random Forest Accuracy: 0.8045\n",
      "Decision Tree Accuracy: 0.7821\n",
      "SVM Accuracy: 0.8212\n",
      "\n",
      "===== Comparison of Accuracy with and without Scaling =====\n",
      "                 Model  Accuracy_Unscaled  Accuracy_Scaled\n",
      "0                  KNN           0.709497         0.804469\n",
      "1  Logistic Regression           0.810056         0.810056\n",
      "2          Naive Bayes           0.770950         0.770950\n",
      "3        Random Forest           0.804469         0.804469\n",
      "4        Decision Tree           0.782123         0.782123\n",
      "5                  SVM           0.653631         0.821229\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"SVM\": SVC(kernel=\"rbf\", C=1.0, gamma=\"scale\")\n",
    "}\n",
    "\n",
    "# Function to train and evaluate models\n",
    "def evaluate_models(X_train, X_test, y_train, y_test, data_type):\n",
    "    print(f\"\\n===== Evaluating Models on {data_type} Data =====\")\n",
    "    results = []\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        results.append({\"Model\": name, \"Accuracy\": accuracy})\n",
    "        print(f\"{name} Accuracy: {accuracy:.4f}\")\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Evaluate models on unscaled data\n",
    "results_unscaled = evaluate_models(X_train, X_test, y_train, y_test, \"Unscaled\")\n",
    "\n",
    "# Evaluate models on scaled data\n",
    "results_scaled = evaluate_models(X_train_scale, X_test_scale, y_train, y_test, \"Scaled\")\n",
    "\n",
    "# Compare results\n",
    "comparison = results_unscaled.merge(results_scaled, on=\"Model\", suffixes=(\"_Unscaled\", \"_Scaled\"))\n",
    "\n",
    "print(\"\\n===== Comparison of Accuracy with and without Scaling =====\")\n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
